{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install einops"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xw89h1QwYB05",
        "outputId": "3e7e0ed8-982e-4a90-b92a-041b085e2c84"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting einops\n",
            "  Downloading einops-0.4.1-py3-none-any.whl (28 kB)\n",
            "Installing collected packages: einops\n",
            "Successfully installed einops-0.4.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "djpC6FPTXv_9"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "import torch.functional as F\n",
        "import numpy as np\n",
        "\n",
        "from einops import rearrange, repeat\n",
        "from einops.layers.torch import Rearrange"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class MultiHeadedSelfAttention(nn.Module):\n",
        "    def __init__(self, dim, num_heads, dropout):\n",
        "        super().__init__()\n",
        "        self.proj_q = nn.Linear(dim, dim)\n",
        "        self.proj_k = nn.Linear(dim, dim)\n",
        "        self.proj_v = nn.Linear(dim, dim)\n",
        "        self.drop = nn.Dropout(dropout)\n",
        "        self.n_heads = num_heads\n",
        "        self.scores = None\n",
        "\n",
        "    def forward(self, x, mask):\n",
        "        #(b s d) -> (b, s d)\n",
        "        q, k, v = self.proj_q(x), self.proj_k(x), self.proj_v(x)\n",
        "        #(b s d) -> (b h s w), h*w = d\n",
        "        q, k, v = map(lambda t : rearrange(t, 'b s (h w) -> b h s w', h=self.n_heads), (q,k,v))\n",
        "\n",
        "        #(b h s w) @ (b h w s) -> (b h s s) -softmax-> (b h s s)\n",
        "        scores = q @ k.transpose(-2, -1) / np.sqrt(k.size(-1))\n",
        "        if mask is not None:\n",
        "            mask = mask[:, None, None, :].float()\n",
        "            scores -= 10000.0 * (1.0 - mask)\n",
        "        scores = self.drop(torch.softmax(scores, dim=-1))\n",
        "\n",
        "        #(b h s s) @ (b h s w) -> (b h s w) -trans-> (b s d)\n",
        "        h = scores @ v\n",
        "        h = rearrange(h, \"b h s w -> b s (h w)\")\n",
        "        self.scores = scores\n",
        "        return h"
      ],
      "metadata": {
        "id": "vDTrPxbydSWZ"
      },
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class PositionWiseFeedForward(nn.Module):\n",
        "    def __init__(self, dim, ff_dim):\n",
        "        super().__init__()\n",
        "        self.fc1 = nn.Linear(dim, ff_dim)\n",
        "        self.fc2 = nn.Linear(ff_dim, dim)\n",
        "        self.gelu = nn.GELU()\n",
        "    def forward(self, x):\n",
        "        return self.fc2(self.gelu(self.fc1(x)))"
      ],
      "metadata": {
        "id": "jRGswKeieim0"
      },
      "execution_count": 64,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Block(nn.Module):\n",
        "    def __init__(self, dim, num_heads, ff_dim, dropout):\n",
        "        super().__init__()\n",
        "        self.attn = MultiHeadedSelfAttention(dim, num_heads, dropout)\n",
        "        self.proj = nn.Linear(dim, dim)\n",
        "        self.norm1 = nn.LayerNorm(dim, eps=1e-6)\n",
        "        self.pwff = PositionWiseFeedForward(dim, ff_dim)\n",
        "        self.norm2 = nn.LayerNorm(dim, eps=1e-6)\n",
        "        self.drop = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x, mask):\n",
        "        h = self.drop(self.proj(self.attn(self.norm1(x), mask)))\n",
        "        x = x + h\n",
        "        h = self.drop(self.pwff(self.norm2(x)))\n",
        "        x = x + h\n",
        "        return x"
      ],
      "metadata": {
        "id": "B5Az9iczjSXY"
      },
      "execution_count": 65,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Transformer(nn.Module):\n",
        "    def __init__(self, num_layers, dim, num_heads, ff_dim, dropout):\n",
        "        super().__init__()\n",
        "        self.blocks = nn.ModuleList([\n",
        "            Block(dim, num_heads, ff_dim, dropout) for _ in range(num_layers)\n",
        "        ])\n",
        "\n",
        "    def forward(self, x, mask=None):\n",
        "        for block in self.blocks:\n",
        "            x = block(x, mask)\n",
        "        return x"
      ],
      "metadata": {
        "id": "daEMiIGfkjDC"
      },
      "execution_count": 66,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class PositionalEmbedding1D(nn.Module):\n",
        "    def __init__(self, seq_len, dim):\n",
        "        super().__init__()\n",
        "        self.pos_embedding = nn.Parameter(torch.zeros(1, seq_len, dim))\n",
        "    def forward(self, x):\n",
        "        return x + self.pos_embedding"
      ],
      "metadata": {
        "id": "RYimvGfCozoe"
      },
      "execution_count": 67,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def pair(t):\n",
        "    return t if isinstance(t, tuple) else (t,t)\n",
        "\n",
        "class ViT(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        image_size,\n",
        "        patch_size,\n",
        "        num_classes,\n",
        "        in_channels,\n",
        "        dim,\n",
        "        ff_dim,\n",
        "        num_heads,\n",
        "        num_layers,\n",
        "        attention_dropout_rate=0.0,\n",
        "        dropout_rate=0.1,\n",
        "        representation_size=None,\n",
        "        load_repr_layer=False,\n",
        "        classifier='token',\n",
        "        positional_embedding='1d',\n",
        "        pretrained=False\n",
        "    ):\n",
        "        super().__init__()\n",
        "        h, w = pair(image_size)\n",
        "        fh, fw = pair(patch_size)\n",
        "        gh, gw = h//fh, w//fw\n",
        "        seq_len = gh * gw\n",
        "\n",
        "        # Patch Embed\n",
        "        self.patch_embedding = nn.Conv2d(in_channels, dim, kernel_size=(fh, fw), stride=(fh, fw))\n",
        "\n",
        "        # CLASS Token\n",
        "        if classifier == 'token':\n",
        "            self.class_token = nn.Parameter(torch.zeros(1, 1, dim))\n",
        "            seq_len += 1\n",
        "        \n",
        "        # Pos Embed\n",
        "        if positional_embedding == '1d':\n",
        "            self.positional_embedding = PositionalEmbedding1D(seq_len, dim)\n",
        "        \n",
        "        # Transformer\n",
        "        self.transformer = Transformer(num_layers, dim, num_heads, ff_dim, dropout_rate)\n",
        "\n",
        "        # Pre Fc Repr Layer\n",
        "        if representation_size and load_repr_layer:\n",
        "            self.pre_logits = nn.Linear(dim, representation_size)\n",
        "            pre_logit_size = representation_size\n",
        "        else:\n",
        "            pre_logit_size = dim\n",
        "        \n",
        "        # Head\n",
        "        self.norm = nn.LayerNorm(pre_logit_size, eps=1e-6)\n",
        "        self.fc = nn.Linear(pre_logit_size, num_classes)\n",
        "    \n",
        "    @torch.no_grad()\n",
        "    def init_weights(self):\n",
        "        def _init(m):\n",
        "            if isinstance(m, nn.Linear):\n",
        "                nn.init.xavier_normal_(m.weight)\n",
        "                if hasattr(m, 'bias') and m.bias is not None:\n",
        "                    nn.init.normal_(m.bias, std=1e-6)\n",
        "        self.apply(_init)\n",
        "        nn.init.constant_(self.fc.weight, 0)\n",
        "        nn.init.constant_(self.fc.bias, 0)\n",
        "        nn.init.normal_(self.positional_embedding.pos_embedding, std=0.02)\n",
        "        nn.init.constant_(self.class_token, 0)\n",
        "    \n",
        "    def forward(self, x):\n",
        "        b, c, fh, fw = x.shape\n",
        "        # (b c fh fw) -patch_embed-> (b d gh gw)\n",
        "        x = self.patch_embedding(x)\n",
        "        # (b d gh gw) -> (b s d)\n",
        "        x = rearrange(x, 'b d gh gw -> b (gh gw) d')\n",
        "        if hasattr(self, 'class_token'):\n",
        "            # (b s d) -> (b s+1 d)\n",
        "            x = torch.cat((repeat(self.class_token, '1 1 d -> b 1 d', b=b), x), dim=1)\n",
        "        x = self.transformer(x)\n",
        "        if hasattr(self, 'pre_logits'):\n",
        "            x = self.pre_logits(x)\n",
        "            x = torch.tanh(x)\n",
        "        if hasattr(self, 'fc'):\n",
        "            x = self.norm(x)[:, 0]\n",
        "            x = self.fc(x)\n",
        "        return x"
      ],
      "metadata": {
        "id": "C2eJ4Ed0afRQ"
      },
      "execution_count": 102,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils import model_zoo\n",
        "\n",
        "vit_b16_config = dict(\n",
        "    image_size=(224, 224),\n",
        "    patch_size=(16, 16),\n",
        "    num_classes=21843,\n",
        "    in_channels=3,\n",
        "    dim=768,\n",
        "    ff_dim=3072,\n",
        "    num_heads=12,\n",
        "    num_layers=12,\n",
        "    attention_dropout_rate=0.0,\n",
        "    dropout_rate=0.1,\n",
        "    representation_size=768,\n",
        "    classifier='token',\n",
        "    pre_trained_url=\"https://github.com/lukemelas/PyTorch-Pretrained-ViT/releases/download/0.0.2/B_16.pth\"\n",
        ")\n",
        "if 'pre_trained_url' in vit_b16_config.keys():\n",
        "    url = vit_b16_config.pop('pre_trained_url')\n",
        "    state_dict = model_zoo.load_url(url)\n",
        "vit_b16 = ViT(**vit_b16_config)\n",
        "ret = vit_b16.load_state_dict(state_dict)"
      ],
      "metadata": {
        "id": "Zi-PZ2iLeFA4"
      },
      "execution_count": 103,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torchvision\n",
        "import torchvision.transforms as transforms"
      ],
      "metadata": {
        "id": "MAtPJ74JzdZ3"
      },
      "execution_count": 104,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "transform = transforms.Compose([\n",
        "    transforms.Resize(224),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize([0.5, 0.5, 0.5], [0.5, 0.5, 0.5])\n",
        "])\n",
        "\n",
        "train_ds = torchvision.datasets.CIFAR100(root='/content/', download=True, transform=transform, train=True)\n",
        "train_ds, valid_ds = torch.utils.data.random_split(train_ds, [40000, 10000])\n",
        "\n",
        "train_loader = torch.utils.data.DataLoader(train_ds,                                          \n",
        "                                          batch_size=64,\n",
        "                                          shuffle=True,\n",
        "                                          num_workers=2)\n",
        "\n",
        "valid_loader = torch.utils.data.DataLoader(valid_ds,\n",
        "                                           batch_size=64,\n",
        "                                           shuffle=False,\n",
        "                                           num_workers=2)\n",
        "\n",
        "test_ds = torchvision.datasets.CIFAR100(root='/content/', download=True, transform=transform, train=False)\n",
        "test_loader = torch.utils.data.DataLoader(test_ds,                                          \n",
        "                                          batch_size=64,\n",
        "                                          shuffle=False,\n",
        "                                          num_workers=2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Oijwpe3ezga7",
        "outputId": "7aa8431c-b552-4629-8524-21155760dfc0"
      },
      "execution_count": 106,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "vit_b16 = vit_b16.to(device)\n",
        "\n",
        "optim = torch.optim.SGD(vit_b16.parameters(),1e-3, momentum=0.9)\n",
        "criterion = torch.nn.CrossEntropyLoss()\n",
        "epochs = 30\n",
        "for epoch in range(epochs):\n",
        "    running_loss = 0.0\n",
        "    for i, (x, y) in enumerate(train_loader, 0):\n",
        "        if epoch == 0 and i == 0:\n",
        "            break\n",
        "        x = x.to(device)\n",
        "        y = y.to(device)\n",
        "\n",
        "        optim.zero_grad()\n",
        "        y_pred = vit_b16(x)\n",
        "        loss = criterion(y_pred, y)\n",
        "        loss.backward()\n",
        "        optim.step()\n",
        "\n",
        "        running_loss += loss.item()\n",
        "        if i % 100 == 99:    # print every 2000 mini-batches\n",
        "            print(f'[{epoch + 1}, {i + 1:5d}] loss: {running_loss / 100:.3f}')\n",
        "            running_loss = 0.0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        valid_loss = 0.0    \n",
        "        total = 0\n",
        "        correct = 0\n",
        "        for x, y in valid_loader:\n",
        "            x = x.to(device)\n",
        "            y = y.to(device)\n",
        "\n",
        "            y_pred = vit_b16(x)\n",
        "            loss = criterion(y_pred, y)\n",
        "            valid_loss += loss.item()\n",
        "            _, predicted = torch.max(y_pred.data, 1)\n",
        "            total += y.size(0)\n",
        "            correct += (predicted == y).sum().item()\n",
        "        print(\"Acc : \", 100*correct//total, \"Loss : \", valid_loss)"
      ],
      "metadata": {
        "id": "pJzJjACe14Qo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "M3Me__ojAib3"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}